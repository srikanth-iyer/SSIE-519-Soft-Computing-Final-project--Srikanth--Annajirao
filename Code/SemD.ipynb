{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paper:\n",
    "Paul Hoffman, Matthew A. Lambon Ralph, and Timothy T. Rogers, “Semantic Diversity: A Measure of Semantic Ambiguity Based on Variability in the Contextual Usage of Words,” Behavior Research Methods 45, no. 3 (September 1, 2013): 718–30, https://doi.org/10.3758/s13428-012-0278-x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lsa(corpus, context_length=1000):\n",
    "    \"\"\"\n",
    "    Perform latent semantic analysis on the given corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list of lists): The corpus of text, where each inner list represents a context.\n",
    "        context_length (int): The length of each context in words. Default is 1,000.\n",
    "    \n",
    "    Returns:\n",
    "        word_vectors (dict): A dictionary mapping words to their 300-dimensional LSA vectors.\n",
    "        context_vectors (list): A list of 300-dimensional LSA vectors, one for each context.\n",
    "    \"\"\"\n",
    "    # Create the co-occurrence matrix\n",
    "    word_to_index = {}\n",
    "    index_to_word = []\n",
    "    co_occurrence_matrix = []\n",
    "    current_context = []\n",
    "    for word in corpus:\n",
    "        current_context.append(word)\n",
    "        if len(current_context) == context_length:\n",
    "            context_vector = [0] * len(set(corpus))\n",
    "            for w in current_context:\n",
    "                if w not in word_to_index:\n",
    "                    word_to_index[w] = len(index_to_word)\n",
    "                    index_to_word.append(w)\n",
    "                word_index = word_to_index[w]\n",
    "                context_vector[word_index] += 1\n",
    "            co_occurrence_matrix.append(context_vector)\n",
    "            current_context = []\n",
    "    if current_context:\n",
    "        context_vector = [0] * len(set(corpus))\n",
    "        for w in current_context:\n",
    "            if w not in word_to_index:\n",
    "                word_to_index[w] = len(index_to_word)\n",
    "                index_to_word.append(w)\n",
    "            word_index = word_to_index[w]\n",
    "            context_vector[word_index] += 1\n",
    "        co_occurrence_matrix.append(context_vector)\n",
    "    co_occurrence_matrix = np.array(co_occurrence_matrix)\n",
    "    \n",
    "    # Apply log transformation and entropy weighting\n",
    "    co_occurrence_matrix = np.log(co_occurrence_matrix + 1)\n",
    "    word_entropies = np.sum(-co_occurrence_matrix * np.log(co_occurrence_matrix), axis=0)\n",
    "    co_occurrence_matrix = np.divide(co_occurrence_matrix, word_entropies)\n",
    "    co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n",
    "    \n",
    "    # Perform singular value decomposition\n",
    "    u, s, vt = np.linalg.svd(co_occurrence_matrix, full_matrices=False)\n",
    "    word_vectors = {word: u[i, :300] for i, word in enumerate(index_to_word)} # 300 dimensions is supposedly optimal according to the article.\n",
    "    context_vectors = vt[:300, :]\n",
    "    \n",
    "    return word_vectors, context_vectors\n",
    "\n",
    "def calculate_semd(word, word_vectors, context_vectors):\n",
    "    \"\"\"\n",
    "    Calculate the semantic diversity (SemD) of the given word.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word for which to calculate SemD.\n",
    "        word_vectors (dict): A dictionary mapping words to their 300-dimensional LSA vectors.\n",
    "        context_vectors (list): A list of 300-dimensional LSA vectors, one for each context.\n",
    "    \n",
    "    Returns:\n",
    "        semd (float): The semantic diversity value for the word.\n",
    "    \"\"\"\n",
    "    # Find all contexts containing the word\n",
    "    word_contexts = [i for i in context_vectors if word in word_vectors]\n",
    "    # if len(word_contexts) > 2000:\n",
    "    #     word_contexts = np.random.choice(word_contexts, size=2000, replace=False)\n",
    "\n",
    "    # Calculate the average cosine similarity between the contexts\n",
    "    context_similarities = [1 - cosine(word_contexts[i], word_contexts[j]) for i in range(len(word_contexts)) for j in range(len(word_contexts)) if i < j]\n",
    "    mean_similarity = np.mean(context_similarities)\n",
    "\n",
    "    # Calculate the SemD value\n",
    "    semd = -np.log(mean_similarity)\n",
    "    \n",
    "    return semd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962134"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "directory = '../Text data/'\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.txt'):\n",
    "        with open(os.path.join(directory, file), 'r') as f:\n",
    "            content = f.read().replace('\\n', ' ')\n",
    "            corpus.extend(content.split())\n",
    "\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split corpus into contexts of length context_length\n",
    "context_length = 200\n",
    "current_context = []\n",
    "contexts = []\n",
    "\n",
    "for word in corpus:\n",
    "    current_context.append(word)\n",
    "    if len(current_context) == context_length:\n",
    "        contexts.append(current_context)\n",
    "        current_context = []\n",
    "\n",
    "if current_context:\n",
    "    contexts.append(current_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_context_embedding(texts):\n",
    "    embeddings = torch.tensor([])\n",
    "    \n",
    "    for text in texts:\n",
    "        # Tokenize the input text\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        \n",
    "        # Pass the input through the BERT model\n",
    "        output = model(**encoded_input)\n",
    "        \n",
    "        # Extract the sentence embedding from the output\n",
    "        context_embedding = output.pooler_output\n",
    "        \n",
    "        embeddings = torch.cat((embeddings, context_embedding.unsqueeze(0).detach()), dim=0)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "all_context_embeddings = get_context_embedding(contexts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_context = []\n",
    "word_to_index = {}\n",
    "index_to_word = []\n",
    "co_occurrence_matrix = []\n",
    "# co_occurrence_matrix = np.zeros((len(set(corpus)), len(set(corpus))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for word in corpus:\n",
    "#     current_context.append(word)\n",
    "#     if len(current_context) == context_length:\n",
    "#         context_vector = [0] * len(set(corpus))\n",
    "#         for w in current_context:\n",
    "#             if w not in word_to_index:\n",
    "#                 word_to_index[w] = len(index_to_word)\n",
    "#                 index_to_word.append(w)\n",
    "#             word_index = word_to_index[w]\n",
    "#             context_vector[word_index] += 1\n",
    "#         contexts.append(current_context)\n",
    "#         co_occurrence_matrix.append(context_vector)\n",
    "#         current_context = []\n",
    "# if current_context:\n",
    "#         context_vector = [0] * len(set(corpus))\n",
    "#         for w in current_context:\n",
    "#             if w not in word_to_index:\n",
    "#                 word_to_index[w] = len(index_to_word)\n",
    "#                 index_to_word.append(w)\n",
    "#             word_index = word_to_index[w]\n",
    "#             context_vector[word_index] += 1\n",
    "#         co_occurrence_matrix.append(context_vector)\n",
    "# co_occurrence_matrix = np.array(co_occurrence_matrix)\n",
    "# co_occurrence_matrix[0]\n",
    "# # Apply log transformation and entropy weighting\n",
    "# co_occurrence_matrix = np.log(co_occurrence_matrix + 1)\n",
    "# word_entropies = np.sum(-co_occurrence_matrix * np.log(co_occurrence_matrix), axis=0)\n",
    "# co_occurrence_matrix = np.divide(co_occurrence_matrix, word_entropies)\n",
    "# co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n",
    "\n",
    "# for context in co_occurrence_matrix:\n",
    "    \n",
    "#     # Perform singular value decomposition\n",
    "#     u[context], s[context], vt[context] = np.linalg.svd(co_occurrence_matrix, full_matrices=False)\n",
    "\n",
    "#     word_vectors = {word: u[i, :300] for i, word in enumerate(index_to_word)} # 300 dimensions is supposedly optimal according to the article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform latent semantic analysis (LSA) on the contexts\n",
    "# This will give us word vectors and context vectors\n",
    "word_vectors, context_vectors = perform_lsa(corpus, context_length=1000)\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'man'\n",
    "word_contexts = [i for i in context_vectors if word in word_vectors]\n",
    "# if len(word_contexts) > 2000:\n",
    "#     word_contexts = np.random.choice(word_contexts, size=2000, replace=False)\n",
    "\n",
    "# Calculate the average cosine similarity between the contexts\n",
    "context_similarities = [1 - cosine(word_contexts[i], word_contexts[j]) for i in range(len(word_contexts)) for j in range(len(word_contexts)) if i < j]\n",
    "mean_similarity = np.mean(context_similarities)\n",
    "\n",
    "# Calculate the SemD value\n",
    "semd = -np.log(mean_similarity)\n",
    "context_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate semantic diversity (SemD) for each word\n",
    "semd_values = []\n",
    "\n",
    "for word in set(corpus):\n",
    "    semd_value = calculate_semd(word, word_vectors, context_vectors)\n",
    "    semd_values.append(semd_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(semd_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semd_values = [0 if np.isnan(x) else x for x in semd_values]\n",
    "semd_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the distribution of words as a function of semantic diversity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(semd_values, bins=30)\n",
    "plt.xlabel('Semantic Diversity (SemD)')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Distribution of Words by Semantic Diversity')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoftComputing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
