{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paper:\n",
    "Paul Hoffman, Matthew A. Lambon Ralph, and Timothy T. Rogers, “Semantic Diversity: A Measure of Semantic Ambiguity Based on Variability in the Contextual Usage of Words,” Behavior Research Methods 45, no. 3 (September 1, 2013): 718–30, https://doi.org/10.3758/s13428-012-0278-x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lsa(corpus, context_length=1000):\n",
    "    \"\"\"\n",
    "    Perform latent semantic analysis on the given corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list of lists): The corpus of text, where each inner list represents a context.\n",
    "        context_length (int): The length of each context in words. Default is 1,000.\n",
    "    \n",
    "    Returns:\n",
    "        word_vectors (dict): A dictionary mapping words to their 300-dimensional LSA vectors.\n",
    "        context_vectors (list): A list of 300-dimensional LSA vectors, one for each context.\n",
    "    \"\"\"\n",
    "    # Create the co-occurrence matrix\n",
    "    word_to_index = {}\n",
    "    index_to_word = []\n",
    "    co_occurrence_matrix = []\n",
    "    current_context = []\n",
    "    for word in corpus:\n",
    "        current_context.append(word)\n",
    "        if len(current_context) == context_length:\n",
    "            context_vector = [0] * len(set(corpus))\n",
    "            for w in current_context:\n",
    "                if w not in word_to_index:\n",
    "                    word_to_index[w] = len(index_to_word)\n",
    "                    index_to_word.append(w)\n",
    "                word_index = word_to_index[w]\n",
    "                context_vector[word_index] += 1\n",
    "            co_occurrence_matrix.append(context_vector)\n",
    "            current_context = []\n",
    "    if current_context:\n",
    "        context_vector = [0] * len(set(corpus))\n",
    "        for w in current_context:\n",
    "            if w not in word_to_index:\n",
    "                word_to_index[w] = len(index_to_word)\n",
    "                index_to_word.append(w)\n",
    "            word_index = word_to_index[w]\n",
    "            context_vector[word_index] += 1\n",
    "        co_occurrence_matrix.append(context_vector)\n",
    "    co_occurrence_matrix = np.array(co_occurrence_matrix)\n",
    "    \n",
    "    # Apply log transformation and entropy weighting\n",
    "    co_occurrence_matrix = np.log(co_occurrence_matrix + 1)\n",
    "    word_entropies = np.sum(-co_occurrence_matrix * np.log(co_occurrence_matrix), axis=0)\n",
    "    co_occurrence_matrix = np.divide(co_occurrence_matrix, word_entropies)\n",
    "    co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n",
    "    \n",
    "    # Perform singular value decomposition\n",
    "    u, s, vt = np.linalg.svd(co_occurrence_matrix, full_matrices=False)\n",
    "    word_vectors = {word: u[i, :300] for i, word in enumerate(index_to_word)} # 300 dimensions is supposedly optimal according to the article.\n",
    "    context_vectors = vt[:300, :]\n",
    "    \n",
    "    return word_vectors, context_vectors\n",
    "\n",
    "def calculate_semd(word, word_vectors, context_vectors):\n",
    "    \"\"\"\n",
    "    Calculate the semantic diversity (SemD) of the given word.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word for which to calculate SemD.\n",
    "        word_vectors (dict): A dictionary mapping words to their 300-dimensional LSA vectors.\n",
    "        context_vectors (list): A list of 300-dimensional LSA vectors, one for each context.\n",
    "    \n",
    "    Returns:\n",
    "        semd (float): The semantic diversity value for the word.\n",
    "    \"\"\"\n",
    "    # Find all contexts containing the word\n",
    "    word_contexts = [i for i in context_vectors if word in word_vectors]\n",
    "    # if len(word_contexts) > 2000:\n",
    "    #     word_contexts = np.random.choice(word_contexts, size=2000, replace=False)\n",
    "\n",
    "    # Calculate the average cosine similarity between the contexts\n",
    "    context_similarities = [1 - cosine(word_contexts[i], word_contexts[j]) for i in range(len(word_contexts)) for j in range(len(word_contexts)) if i < j]\n",
    "    mean_similarity = np.mean(context_similarities)\n",
    "\n",
    "    # Calculate the SemD value\n",
    "    semd = -np.log(mean_similarity)\n",
    "    \n",
    "    return semd\n",
    "\n",
    "def prepare_text(text):\n",
    "    # text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = torch.ones((1, len(indexed_tokens)), dtype=torch.long)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = segments_ids.clone()\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Corpus: 1190921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Midsummer',\n",
       " 'Night',\n",
       " \"'s\",\n",
       " 'Dream',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " 'Edited',\n",
       " 'by',\n",
       " 'Barbara',\n",
       " 'A.',\n",
       " 'Mowat',\n",
       " 'and',\n",
       " 'Paul',\n",
       " 'Werstine',\n",
       " 'with',\n",
       " 'Michael',\n",
       " 'Poston',\n",
       " 'and']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "directory = '../Text data/'\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.txt'):\n",
    "        with open(os.path.join(directory, file), 'r') as f:\n",
    "            content = f.read().replace('\\n', ' ')\n",
    "            # Remove website links from the corpus\n",
    "            corpus_without_links = [re.sub(r'http[s]?://\\S+', '', text) for text in corpus]\n",
    "            # content = tokenizer.tokenize(content) # Bert Tokenizer  will not preserve the original words\n",
    "            content = word_tokenize(content) # NLTK Tokenizer will preserve the original words, but I'm not sure if they can be used with BERT\n",
    "            corpus.extend(content)\n",
    "\n",
    "\n",
    "print(f\"Length of Corpus: {len(corpus)}\")\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split corpus into contexts of length context_length\n",
    "context_length = 100\n",
    "current_context = []\n",
    "contexts = []\n",
    "\n",
    "for word in corpus:\n",
    "    current_context.append(word)\n",
    "    if len(current_context) == context_length:\n",
    "        contexts.append(current_context)\n",
    "        current_context = []\n",
    "\n",
    "if current_context:\n",
    "    contexts.append(current_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_co_occurrence_matrix(corpus, window_size=1):\n",
    "    distinct_words = sorted(list(set(corpus)))\n",
    "    num_words = len(distinct_words)\n",
    "    \n",
    "    # Create word to index and index to word mappings\n",
    "    word_to_index = {word: index for index, word in enumerate(distinct_words)}\n",
    "    index_to_word = {index: word for index, word in enumerate(distinct_words)}\n",
    "    \n",
    "    # Create an empty co-occurrence matrix\n",
    "    co_occurrence_matrix = np.zeros((num_words, num_words))\n",
    "    \n",
    "    # Iterate over the corpus and update the co-occurrence matrix\n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(max(0, i - window_size), min(i + window_size, len(corpus) - 1) + 1):\n",
    "            if i != j:\n",
    "                co_occurrence_matrix[word_to_index[corpus[i]]][word_to_index[corpus[j]]] += 1\n",
    "                \n",
    "    return co_occurrence_matrix, word_to_index, index_to_word\n",
    "\n",
    "# Example usage:\n",
    "corpus = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "co_occurrence_matrix, word_to_index, index_to_word = create_co_occurrence_matrix(corpus, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(contexts):\n",
    "\n",
    "\n",
    "    all_context_embeddings = torch.tensor([])\n",
    "    all_word_embeddings = torch.tensor([])\n",
    "\n",
    "    for context in contexts:\n",
    "        # Tokenize the input text\n",
    "        encoded_input = tokenizer(context, return_tensors='pt', padding=True, truncation=True)\n",
    "        \n",
    "        # Pass the input through the BERT model\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded_input)\n",
    "        \n",
    "        # Extract the context embedding from the output\n",
    "        context_embedding = output.pooler_output\n",
    "        word_embeddings = output.last_hidden_state\n",
    "\n",
    "        print(word_embeddings.shape)\n",
    "\n",
    "        all_context_embeddings = torch.cat((all_context_embeddings, context_embedding.unsqueeze(0).detach()), dim=0)\n",
    "        all_word_embeddings = torch.cat((all_word_embeddings, word_embeddings.detach()), dim=0)\n",
    "\n",
    "        print(f'Context {i + 1} out of {len(contexts[:10])} completed')\n",
    "        \n",
    "    # removing embeddings for the tokens [CLS] and [SEP]\n",
    "    all_word_embeddings = all_word_embeddings[:, 1:-1, :]\n",
    "\n",
    "    \n",
    "    return all_context_embeddings, all_word_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 28, 768])\n",
      "Context 2 out of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100, 768]), torch.Size([100, 26, 768]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Example list of contexts\n",
    "sample_contexts = contexts[:1]\n",
    "\n",
    "all_context_embeddings = torch.tensor([])\n",
    "all_word_embeddings = torch.tensor([])\n",
    "\n",
    "all_context_embeddings, all_word_embeddings = get_embedding(sample_contexts)\n",
    "\n",
    "all_context_embeddings.shape, all_word_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co occurance matrix shape: (1, 34157)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srikanth\\AppData\\Local\\Temp\\ipykernel_7452\\1904785255.py:43: RuntimeWarning: divide by zero encountered in log\n",
      "  word_entropies = np.sum(-co_occurrence_matrix * np.log(co_occurrence_matrix), axis=0)\n",
      "C:\\Users\\Srikanth\\AppData\\Local\\Temp\\ipykernel_7452\\1904785255.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  word_entropies = np.sum(-co_occurrence_matrix * np.log(co_occurrence_matrix), axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Create the co-occurrence matrix\n",
    "word_to_index = {}\n",
    "index_to_word = []\n",
    "co_occurrence_matrix = []\n",
    "current_context = []\n",
    "# for word in corpus:\n",
    "#     current_context.append(word)\n",
    "#     if len(current_context) == context_length:\n",
    "#         context_vector = [0] * len(set(corpus))\n",
    "#         for w in current_context:\n",
    "#             if w not in word_to_index:\n",
    "#                 word_to_index[w] = len(index_to_word)\n",
    "#                 index_to_word.append(w)\n",
    "#             word_index = word_to_index[w]\n",
    "#             context_vector[word_index] += 1\n",
    "#         co_occurrence_matrix.append(context_vector)\n",
    "#         current_context = []\n",
    "# if current_context:\n",
    "#     context_vector = [0] * len(set(corpus))\n",
    "#     for w in current_context:\n",
    "#         if w not in word_to_index:\n",
    "#             word_to_index[w] = len(index_to_word)\n",
    "#             index_to_word.append(w)\n",
    "#         word_index = word_to_index[w]\n",
    "#         context_vector[word_index] += 1\n",
    "#     co_occurrence_matrix.append(context_vector)\n",
    "# co_occurrence_matrix = np.array(co_occurrence_matrix)\n",
    "\n",
    "# Generating co-occurrence matrix for each context in contexts\n",
    "for context in contexts[:1]:\n",
    "    context_vector = [0] * len(set(corpus))\n",
    "    for w in context:\n",
    "        if w not in word_to_index:\n",
    "            word_to_index[w] = len(index_to_word)\n",
    "            index_to_word.append(w)\n",
    "        word_index = word_to_index[w]\n",
    "        context_vector[word_index] += 1\n",
    "    co_occurrence_matrix.append(context_vector)\n",
    "co_occurrence_matrix = np.array(co_occurrence_matrix)\n",
    "print(f'Co occurance matrix shape: {co_occurrence_matrix.shape}')\n",
    "# Apply log transformation and entropy weighting\n",
    "co_occurrence_matrix = np.log(co_occurrence_matrix + 1)\n",
    "word_entropies = np.sum(-co_occurrence_matrix * np.log(co_occurrence_matrix), axis=0)\n",
    "co_occurrence_matrix = np.divide(co_occurrence_matrix, word_entropies)\n",
    "co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "1-dimensional array given. Array must be at least two-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[179], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(co_occurrence_matrix)):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Perform singular value decomposition\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     u, s, vt \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mco_occurrence_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mu\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVt shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvt\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Srikanth\\.conda\\envs\\SoftComputing\\Lib\\site-packages\\numpy\\linalg\\linalg.py:1662\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[0;32m   1659\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(s)\n\u001b[0;32m   1660\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m sort(s)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1662\u001b[0m \u001b[43m_assert_stacked_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1663\u001b[0m t, result_t \u001b[38;5;241m=\u001b[39m _commonType(a)\n\u001b[0;32m   1665\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_svd_nonconvergence)\n",
      "File \u001b[1;32mc:\\Users\\Srikanth\\.conda\\envs\\SoftComputing\\Lib\\site-packages\\numpy\\linalg\\linalg.py:206\u001b[0m, in \u001b[0;36m_assert_stacked_2d\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-dimensional array given. Array must be \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    207\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat least two-dimensional\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m a\u001b[38;5;241m.\u001b[39mndim)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: 1-dimensional array given. Array must be at least two-dimensional"
     ]
    }
   ],
   "source": [
    "for i in range(len(co_occurrence_matrix)):\n",
    "    # Perform singular value decomposition\n",
    "    u, s, vt = np.linalg.svd(co_occurrence_matrix[i], full_matrices=False)\n",
    "    print(f'U shape: {u.shape}')\n",
    "    print(f'Vt shape: {vt.shape}')\n",
    "    print(f'S shape: {s.shape}')\n",
    "# word_vectors = {word: u[i, :300] for i, word in enumerate(index_to_word)} # 300 dimensions is supposedly optimal according to the article.\n",
    "# context_vectors = vt[:300, :]\n",
    "# word_vectors = {index_to_word[i]: u[i] for i in range(len(index_to_word))}\n",
    "u.shape\n",
    "# word_to_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 34157)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(co_occurrence_matrix), len(co_occurrence_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform latent semantic analysis (LSA) on the contexts\n",
    "# This will give us word vectors and context vectors\n",
    "word_vectors, context_vectors = perform_lsa(corpus, context_length=1000)\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'man'\n",
    "word_contexts = [i for i in context_vectors if word in word_vectors]\n",
    "# if len(word_contexts) > 2000:\n",
    "#     word_contexts = np.random.choice(word_contexts, size=2000, replace=False)\n",
    "\n",
    "# Calculate the average cosine similarity between the contexts\n",
    "context_similarities = [1 - cosine(word_contexts[i], word_contexts[j]) for i in range(len(word_contexts)) for j in range(len(word_contexts)) if i < j]\n",
    "mean_similarity = np.mean(context_similarities)\n",
    "\n",
    "# Calculate the SemD value\n",
    "semd = -np.log(mean_similarity)\n",
    "context_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate semantic diversity (SemD) for each word\n",
    "semd_values = []\n",
    "\n",
    "for word in set(corpus):\n",
    "    semd_value = calculate_semd(word, word_vectors, context_vectors)\n",
    "    semd_values.append(semd_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(semd_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semd_values = [0 if np.isnan(x) else x for x in semd_values]\n",
    "semd_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the distribution of words as a function of semantic diversity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(semd_values, bins=30)\n",
    "plt.xlabel('Semantic Diversity (SemD)')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Distribution of Words by Semantic Diversity')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoftComputing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
