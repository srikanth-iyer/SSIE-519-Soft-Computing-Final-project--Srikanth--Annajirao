{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paper:\n",
    "Paul Hoffman, Matthew A. Lambon Ralph, and Timothy T. Rogers, “Semantic Diversity: A Measure of Semantic Ambiguity Based on Variability in the Contextual Usage of Words,” Behavior Research Methods 45, no. 3 (September 1, 2013): 718–30, https://doi.org/10.3758/s13428-012-0278-x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to recreate SemD calculations as per the specifications in the above paper. It doesn't use bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from scipy.spatial import distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Corpus: 953359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Midsummer',\n",
       " 'Night',\n",
       " 'Dream',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " 'Edited',\n",
       " 'by',\n",
       " 'Barbara',\n",
       " 'Mowat',\n",
       " 'and',\n",
       " 'Paul',\n",
       " 'Werstine',\n",
       " 'with',\n",
       " 'Michael',\n",
       " 'Poston',\n",
       " 'and',\n",
       " 'Rebecca',\n",
       " 'Niles']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "directory = '../Text data/'\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.txt'):\n",
    "        with open(os.path.join(directory, file), 'r') as f:\n",
    "            content = f.read().replace('\\n', ' ')\n",
    "            # content = tokenizer.tokenize(content) # Bert Tokenizer  will not preserve the original words\n",
    "            content = word_tokenize(content) # NLTK Tokenizer will preserve the original words, but I'm not sure if they can be used with BERT\n",
    "            corpus.extend(content)\n",
    "# delete tokens that are not words\n",
    "corpus = [word for word in corpus if re.match(r'^[a-zA-Z]+$', word)]\n",
    "\n",
    "print(f\"Length of Corpus: {len(corpus)}\")\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## context splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split corpus into contexts of length context_length\n",
    "context_length = 100\n",
    "current_context = []\n",
    "contexts = []\n",
    "\n",
    "for word in corpus:\n",
    "    current_context.append(word)\n",
    "    if len(current_context) == context_length:\n",
    "        contexts.append(current_context)\n",
    "        current_context = []\n",
    "\n",
    "if current_context:\n",
    "    contexts.append(current_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA def:  perform_lsa(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lsa(corpus):\n",
    "    \"\"\"\n",
    "    Perform latent semantic analysis on the given corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list of lists): The corpus of text, where each inner list represents a context.\n",
    "        context_length (int): The length of each context in words. Default is 1,000.\n",
    "    \n",
    "    Returns:\n",
    "        word_vectors (dict): A dictionary mapping words to their 300-dimensional LSA vectors.\n",
    "        context_vectors (list): A list of 300-dimensional LSA vectors, one for each context.\n",
    "    \"\"\"\n",
    "    co_occurrence_matrix, word_to_index, index_to_word = create_co_occurrence_matrix(corpus)\n",
    "\n",
    "\n",
    "    print(f'Co occurance matrix shape: {co_occurrence_matrix.shape}')\n",
    "    print(f'Word to index mapping length: {len(word_to_index)}')\n",
    "    print(f'Index to word mapping length: {len(index_to_word)}')\n",
    "\n",
    "    # Apply log transformation \n",
    "    co_occurrence_matrix = np.log(co_occurrence_matrix + 1)\n",
    "\n",
    "    # get word entropy of each word in the matrix\n",
    "    word_entropies = np.sum(-co_occurrence_matrix * np.log(co_occurrence_matrix + 1e-10), axis=0) # Add a small value to avoid log(0)\n",
    "\n",
    "    # Apply entropy weighting\n",
    "    co_occurrence_matrix = np.divide(co_occurrence_matrix, word_entropies + 1e-10) # Add a small value to avoid division by zero\n",
    "    co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n",
    "\n",
    "    # Perform singular value decomposition\n",
    "    u, s, vt = np.linalg.svd(co_occurrence_matrix, full_matrices=False)\n",
    "    print(f'U shape: {u.shape}')\n",
    "    print(f'Vt shape: {vt.shape}')\n",
    "    print(f'S shape: {s.shape}')\n",
    "\n",
    "    # word_vectors = {word: u[i, :300] for i, word in enumerate(index_to_word)} # 300 dimensions is supposedly optimal according to the article.\n",
    "    word_vectors = {word: u[i, :] for i, word in enumerate(index_to_word)} # I'm doing all values unlike the article, because I'm not sure if the 300 dimensions are optimal.\n",
    "    # context_vectors = vt[:300, :]\n",
    "    context_vectors = vt[:, :]\n",
    "\n",
    "    print(f'Word vectors length: {len(word_vectors)}')\n",
    "    print(f'Context vectors length: {len(context_vectors)}')\n",
    "    \n",
    "    return word_vectors, context_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemD def:  calculate_semd(word, word_vectors, context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_semd(word, word_vectors, context_vectors):\n",
    "    \"\"\"\n",
    "    Calculate the semantic diversity (SemD) of the given word.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word for which to calculate SemD.\n",
    "        word_vectors (dict): A dictionary mapping words to their 300-dimensional LSA vectors.\n",
    "        context_vectors (list): A list of 300-dimensional LSA vectors, one for each context.\n",
    "    \n",
    "    Returns:\n",
    "        semd (float): The semantic diversity value for the word.\n",
    "    \"\"\"\n",
    "    # Find all contexts containing the word\n",
    "    word_contexts = [i for i in context_vectors if word in word_vectors]\n",
    "    # if len(word_contexts) > 2000:\n",
    "    #     word_contexts = np.random.choice(word_contexts, size=2000, replace=False)\n",
    "\n",
    "    # Calculate the average cosine similarity between the contexts\n",
    "    context_similarities = [1 - distance.cosine(word_contexts[i], word_contexts[j]) for i in range(len(word_contexts)) for j in range(len(word_contexts)) if i < j]\n",
    "    mean_similarity = np.mean(context_similarities)\n",
    "\n",
    "    # Calculate the SemD value\n",
    "    semd = -np.log(mean_similarity)\n",
    "    \n",
    "    return semd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurence matrix def: create_co_occurrence_matrix(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words: 8\n",
      "Number of words in corpus: 9\n",
      "Number of distinct words with at least 1 appearances: 8\n",
      "[[0. 1. 1. 1. 1. 1. 1. 2.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 2.]\n",
      " [1. 1. 0. 1. 1. 1. 1. 2.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 2.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 2.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 2.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2.]]\n",
      "{'brown': 0, 'dog': 1, 'fox': 2, 'jumps': 3, 'lazy': 4, 'over': 5, 'quick': 6, 'the': 7}\n",
      "{0: 'brown', 1: 'dog', 2: 'fox', 3: 'jumps', 4: 'lazy', 5: 'over', 6: 'quick', 7: 'the'}\n"
     ]
    }
   ],
   "source": [
    "def create_co_occurrence_matrix(corpus, window_size=len(corpus)):\n",
    "    distinct_words = sorted(list(set(corpus)))\n",
    "    num_words = len(distinct_words)\n",
    "    \n",
    "    print(f\"Number of distinct words: {num_words}\")\n",
    "    print(f\"Number of words in corpus: {len(corpus)}\")\n",
    "\n",
    "    # Create word to index and index to word mappings\n",
    "    word_to_index = {word: index for index, word in enumerate(distinct_words)}\n",
    "    index_to_word = {index: word for index, word in enumerate(distinct_words)}\n",
    "    \n",
    "    # Create an empty co-occurrence matrix\n",
    "    co_occurrence_matrix = np.zeros((num_words, num_words))\n",
    "    # only words with minimum 10 appearances in corpus are inculeded in the co-occurrence matrix\n",
    "    word_counts = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        word_counts[word] += 1\n",
    "    for i, word in enumerate(distinct_words):\n",
    "        if word_counts[word] >= 1:\n",
    "            word_to_index[word] = i \n",
    "            index_to_word[i] = word\n",
    "        else:\n",
    "            del word_to_index[word]\n",
    "            del index_to_word[i]\n",
    "    num_words = len(word_to_index)\n",
    "    print(f\"Number of distinct words with at least 1 appearances: {num_words}\")\n",
    "\n",
    "    # Iterate over the corpus and update the co-occurrence matrix\n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(max(0, i - window_size), min(i + window_size, len(corpus) - 1) + 1):\n",
    "            if i != j:\n",
    "                if corpus[i] in word_to_index and corpus[j] in word_to_index:\n",
    "                    co_occurrence_matrix[word_to_index[corpus[i]]][word_to_index[corpus[j]]] += 1\n",
    "    \n",
    "    # # Iterate over the corpus and update the co-occurrence matrix\n",
    "    # for i in range(len(corpus)):\n",
    "    #     for j in range(max(0, i - window_size), min(i + window_size, len(corpus) - 1) + 1):\n",
    "    #         if i != j:\n",
    "    #             co_occurrence_matrix[word_to_index[corpus[i]]][word_to_index[corpus[j]]] += 1\n",
    "                \n",
    "    return co_occurrence_matrix, word_to_index, index_to_word\n",
    "\n",
    "# Example usage:\n",
    "samplecorpus = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "co_occurrence_matrix, word_to_index, index_to_word = create_co_occurrence_matrix(samplecorpus)\n",
    "print(co_occurrence_matrix)\n",
    "print(word_to_index)\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating LSA\n",
    "and getting word vectors context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words: 1565\n",
      "Number of words in corpus: 5000\n",
      "Number of distinct words with at least 10 appearances: 87\n",
      "Co occurance matrix shape: (1565, 1565)\n",
      "Word to index mapping length: 87\n",
      "Index to word mapping length: 87\n",
      "U shape: (1565, 1565)\n",
      "Vt shape: (1565, 1565)\n",
      "S shape: (1565,)\n",
      "Word vectors length: 87\n",
      "Context vectors length: 300\n"
     ]
    }
   ],
   "source": [
    "# co_occurrence_matrix, word_to_index, index_to_word = create_co_occurrence_matrix(corpus[:5000])\n",
    "\n",
    "\n",
    "# print(f'Co occurance matrix shape: {co_occurrence_matrix.shape}')\n",
    "# print(f'Word to index mapping length: {len(word_to_index)}')\n",
    "# print(f'Index to word mapping length: {len(index_to_word)}')\n",
    "\n",
    "# # Apply log transformation \n",
    "# co_occurrence_matrix = np.log(co_occurrence_matrix + 1)\n",
    "\n",
    "# # get word entropy of each word in the matrix\n",
    "# word_entropies = np.sum(-co_occurrence_matrix * np.log(co_occurrence_matrix + 1e-10), axis=0) # Add a small value to avoid log(0)\n",
    "\n",
    "# # Apply entropy weighting\n",
    "# co_occurrence_matrix = np.divide(co_occurrence_matrix, word_entropies + 1e-10) # Add a small value to avoid division by zero\n",
    "# co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n",
    "\n",
    "# # Perform singular value decomposition\n",
    "# u, s, vt = np.linalg.svd(co_occurrence_matrix, full_matrices=False)\n",
    "# print(f'U shape: {u.shape}')\n",
    "# print(f'Vt shape: {vt.shape}')\n",
    "# print(f'S shape: {s.shape}')\n",
    "\n",
    "# word_vectors = {word: u[i, :300] for i, word in enumerate(index_to_word)} # 300 dimensions is supposedly optimal according to the article.\n",
    "# context_vectors = vt[:300, :]\n",
    "\n",
    "# print(f'Word vectors length: {len(word_vectors)}')\n",
    "# print(f'Context vectors length: {len(context_vectors)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words: 2368\n",
      "Number of words in corpus: 10000\n",
      "Number of distinct words with at least 1 appearances: 2368\n",
      "Co occurance matrix shape: (2368, 2368)\n",
      "Word to index mapping length: 2368\n",
      "Index to word mapping length: 2368\n",
      "U shape: (2368, 2368)\n",
      "Vt shape: (2368, 2368)\n",
      "S shape: (2368,)\n",
      "Word vectors length: 2368\n",
      "Context vectors length: 2368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2368, 2368)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing LSA for the whole corpus\n",
    "context_vectors, word_vectors = perform_lsa(corpus[:10000])\n",
    "len(word_vectors), len(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_to_index['the']\n",
    "# word_vectors[word_to_index['the']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the co-occurence matrix to a file\n",
    "# np.save('co_occurrence_matrix.npy', co_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co occurance matrix shape: (1, 29634)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srikanth\\AppData\\Local\\Temp\\ipykernel_4212\\2298958237.py:22: RuntimeWarning: divide by zero encountered in log\n",
      "  word_entropies = np.sum(-co_occurrence_matrix * np.log(co_occurrence_matrix), axis=0)\n",
      "C:\\Users\\Srikanth\\AppData\\Local\\Temp\\ipykernel_4212\\2298958237.py:22: RuntimeWarning: invalid value encountered in multiply\n",
      "  word_entropies = np.sum(-co_occurrence_matrix * np.log(co_occurrence_matrix), axis=0)\n"
     ]
    }
   ],
   "source": [
    "# # Create the co-occurrence matrix\n",
    "# word_to_index = {}\n",
    "# index_to_word = []\n",
    "# co_occurrence_matrix = []\n",
    "# current_context = []\n",
    "\n",
    "\n",
    "# # Generating co-occurrence matrix for each context in contexts\n",
    "# for context in contexts[:1]:\n",
    "#     context_vector = [0] * len(set(corpus))\n",
    "#     for w in context:\n",
    "#         if w not in word_to_index:\n",
    "#             word_to_index[w] = len(index_to_word)\n",
    "#             index_to_word.append(w)\n",
    "#         word_index = word_to_index[w]\n",
    "#         context_vector[word_index] += 1\n",
    "#     co_occurrence_matrix.append(context_vector)\n",
    "# co_occurrence_matrix = np.array(co_occurrence_matrix)\n",
    "# print(f'Co occurance matrix shape: {co_occurrence_matrix.shape}')\n",
    "# # Apply log transformation and entropy weighting\n",
    "# co_occurrence_matrix = np.log(co_occurrence_matrix + 1)\n",
    "# word_entropies = np.sum(-co_occurrence_matrix * np.log(co_occurrence_matrix), axis=0)\n",
    "# co_occurrence_matrix = np.divide(co_occurrence_matrix, word_entropies)\n",
    "# co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U shape: (806, 806)\n",
      "Vt shape: (806, 806)\n",
      "S shape: (806,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Perform singular value decomposition\n",
    "# u, s, vt = np.linalg.svd(co_occurrence_matrix, full_matrices=False)\n",
    "# print(f'U shape: {u.shape}')\n",
    "# print(f'Vt shape: {vt.shape}')\n",
    "# print(f'S shape: {s.shape}')\n",
    "# word_vectors = {word: u[i, :300] for i, word in enumerate(index_to_word)} # 300 dimensions is supposedly optimal according to the article.\n",
    "# context_vectors = vt[:300, :]\n",
    "# word_vectors = {index_to_word[i]: u[i] for i in range(len(index_to_word))}\n",
    "# len(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1369"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Perform latent semantic analysis (LSA) on the contexts\n",
    "# # This will give us word vectors and context vectors\n",
    "# word_vectors, context_vectors = perform_lsa(corpus, context_length=1000)\n",
    "# word_vectors\n",
    "word_to_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: A, SemD: 40.19228816291038\n",
      "Word: ACT, SemD: 40.19228816291038\n",
      "Word: ALL, SemD: 40.19228816291038\n",
      "Word: Adieu, SemD: 40.19228816291038\n",
      "Word: Aegles, SemD: 40.19228816291038\n",
      "Word: Against, SemD: 40.19228816291038\n",
      "Word: Ah, SemD: 40.19228816291038\n",
      "Word: All, SemD: 40.19228816291038\n",
      "Word: Am, SemD: 40.19228816291038\n",
      "Word: Amazon, SemD: 40.19228816291038\n",
      "Word: Amazons, SemD: 40.19228816291038\n",
      "Word: An, SemD: 40.19228816291038\n",
      "Word: And, SemD: 40.19228816291038\n",
      "Word: Another, SemD: 40.19228816291038\n",
      "Word: Answer, SemD: 40.19228816291038\n",
      "Word: Antiopa, SemD: 40.19228816291038\n",
      "Word: Apollo, SemD: 40.19228816291038\n",
      "Word: Are, SemD: 40.19228816291038\n",
      "Word: Ariadne, SemD: 40.19228816291038\n",
      "Word: As, SemD: 40.19228816291038\n",
      "Word: At, SemD: 40.19228816291038\n",
      "Word: Athenian, SemD: 40.19228816291038\n",
      "Word: Athens, SemD: 40.19228816291038\n",
      "Word: Attendants, SemD: 40.19228816291038\n",
      "Word: Awake, SemD: 40.19228816291038\n",
      "Word: Ay, SemD: 40.19228816291038\n",
      "Word: BOTTOM, SemD: 40.19228816291038\n",
      "Word: Barbara, SemD: 40.19228816291038\n",
      "Word: Be, SemD: 40.19228816291038\n",
      "Word: Because, SemD: 40.19228816291038\n",
      "Word: Before, SemD: 40.19228816291038\n",
      "Word: Behold, SemD: 40.19228816291038\n",
      "Word: Belike, SemD: 40.19228816291038\n",
      "Word: Beteem, SemD: 40.19228816291038\n",
      "Word: Bootless, SemD: 40.19228816291038\n",
      "Word: Bottom, SemD: 40.19228816291038\n",
      "Word: Brief, SemD: 40.19228816291038\n",
      "Word: But, SemD: 40.19228816291038\n",
      "Word: By, SemD: 40.19228816291038\n",
      "Word: COBWEB, SemD: 40.19228816291038\n",
      "Word: Call, SemD: 40.19228816291038\n",
      "Word: Called, SemD: 40.19228816291038\n",
      "Word: Can, SemD: 40.19228816291038\n",
      "Word: Carthage, SemD: 40.19228816291038\n",
      "Word: Chanting, SemD: 40.19228816291038\n",
      "Word: Characters, SemD: 40.19228816291038\n",
      "Word: Come, SemD: 40.19228816291038\n",
      "Word: Consent, SemD: 40.19228816291038\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[195], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m semd_values \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_to_index\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m----> 4\u001b[0m     semd_values[word] \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_semd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, SemD: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msemd_values[word]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[193], line 19\u001b[0m, in \u001b[0;36mcalculate_semd\u001b[1;34m(word, word_vectors, context_vectors)\u001b[0m\n\u001b[0;32m     14\u001b[0m word_contexts \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m context_vectors \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_vectors]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# if len(word_contexts) > 2000:\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#     word_contexts = np.random.choice(word_contexts, size=2000, replace=False)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate the average cosine similarity between the contexts\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m context_similarities \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mdistance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_contexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_contexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word_contexts)) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word_contexts)) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m j]\n\u001b[0;32m     20\u001b[0m mean_similarity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(context_similarities)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate the SemD value\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Srikanth\\.conda\\envs\\SoftComputing\\Lib\\site-packages\\scipy\\spatial\\distance.py:695\u001b[0m, in \u001b[0;36mcosine\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;124;03mCompute the Cosine distance between 1-D arrays.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    690\u001b[0m \n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;66;03m# cosine distance is also referred to as 'uncentered correlation',\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;66;03m#   or 'reflective correlation'\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;66;03m# clamp the result to 0-2\u001b[39;00m\n\u001b[1;32m--> 695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmin\u001b[39m(\u001b[43mcorrelation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m, \u001b[38;5;241m2.0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Srikanth\\.conda\\envs\\SoftComputing\\Lib\\site-packages\\scipy\\spatial\\distance.py:644\u001b[0m, in \u001b[0;36mcorrelation\u001b[1;34m(u, v, w, centered)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    643\u001b[0m     vw, uw \u001b[38;5;241m=\u001b[39m v, u\n\u001b[1;32m--> 644\u001b[0m uv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    645\u001b[0m uu \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(u, uw)\n\u001b[0;32m    646\u001b[0m vv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(v, vw)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calculate the semantic diversity of all words in the index of words\n",
    "semd_values = {}\n",
    "for word in word_to_index.keys():\n",
    "    semd_values[word] = calculate_semd(word, word_vectors, context_vectors)\n",
    "    print(f\"Word: {word}, SemD: {semd_values[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate semantic diversity (SemD) for each word\n",
    "semd_values = []\n",
    "\n",
    "for word in set(corpus):\n",
    "    semd_value = calculate_semd(word, word_vectors, context_vectors)\n",
    "    semd_values.append(semd_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(semd_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semd_values = [0 if np.isnan(x) else x for x in semd_values]\n",
    "semd_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the distribution of words as a function of semantic diversity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(semd_values, bins=30)\n",
    "plt.xlabel('Semantic Diversity (SemD)')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Distribution of Words by Semantic Diversity')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoftComputing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
