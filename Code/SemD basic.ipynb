{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paper:\n",
    "Paul Hoffman, Matthew A. Lambon Ralph, and Timothy T. Rogers, “Semantic Diversity: A Measure of Semantic Ambiguity Based on Variability in the Contextual Usage of Words,” Behavior Research Methods 45, no. 3 (September 1, 2013): 718–30, https://doi.org/10.3758/s13428-012-0278-x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "This notebook is to recreate SemD calculations as per the specifications in the above paper. It doesn't use bert.\n",
    "\n",
    "It is almost complete, although the SemDvalues don't entirely makes sense yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from scipy.spatial import distance\n",
    "import scipy\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the imported packages to the requirement file\n",
    "# !pip freeze > requirements.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "def load_corpus(directory):\n",
    "    \"\"\"\n",
    "    Load the corpus from the directory and return a list of words\n",
    "    \"\"\"\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    # Use glob to match .txt file paths recursively\n",
    "    file_paths = glob.glob(os.path.join(directory, '**/*.txt'), recursive=True)\n",
    "    \n",
    "\n",
    "    for file_path in file_paths:\n",
    "    \n",
    "        with open(file_path,'r',encoding='utf-8') as f:\n",
    "            content = f.read().replace('\\n', ' ')\n",
    "            # content = tokenizer.tokenize(content) # Bert Tokenizer  will not preserve the original words\n",
    "            content = word_tokenize(content) # NLTK Tokenizer will preserve the original words, but I'm not sure if they can be used with BERT\n",
    "\n",
    "            # make all words lowercase\n",
    "            content = [word.lower() for word in content]\n",
    "            corpus.extend(content)\n",
    "\n",
    "    # delete tokens that are not words\n",
    "    corpus = [word for word in corpus if re.match(r'^[a-zA-Z]+$', word)]\n",
    "    print(f'Length of Corpus: {len(corpus)}')\n",
    "    print(f'First 20 words in the corpus: {corpus[:20]}')\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## context splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_splitting(corpus, context_length):\n",
    "    \"\"\"\n",
    "    Split the corpus into contexts of length context_length\n",
    "    \"\"\"\n",
    "    \n",
    "    current_context = []\n",
    "    contexts = []\n",
    "\n",
    "    for word in corpus:\n",
    "        current_context.append(word)\n",
    "        if len(current_context) == context_length:\n",
    "            contexts.append(current_context)\n",
    "            current_context = []\n",
    "\n",
    "    if current_context:\n",
    "        contexts.append(current_context)\n",
    "    \n",
    "    return contexts\n",
    "\n",
    "contexts = context_splitting(corpus, context_length = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurence matrix def: create_co_occurrence_matrix(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_occurrence_matrix(corpus, contexts, min_frequency = 5):\n",
    "    '''\n",
    "    Create a co-occurrence matrix based on the given corpus and contexts.\n",
    "\n",
    "    Parameters:\n",
    "    - corpus (list): A list of words representing the corpus.\n",
    "    - contexts (list): A list of lists, where each inner list represents a context of default length 1000 words unless specified otherwise.\n",
    "    - min_frequency (int): The minimum number of times a word must appear in the corpus to be included in the co-occurrence matrix.\n",
    "\n",
    "    Returns:\n",
    "    - co_occurrence_matrix (numpy.ndarray): The rows represent words and the columns represent contexts. The value at (i, j) represents the number of times word i appears in context j.\n",
    "\n",
    "    - word_to_index (dict): A dictionary mapping words to their corresponding indices in the matrix.\n",
    "    \n",
    "    - index_to_word (dict): A dictionary mapping indices to their corresponding words in the matrix.\n",
    "\n",
    "    Example usage:\n",
    "    corpus = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "    contexts = [[\"the\", \"quick\", \"brown\"], [\"fox\", \"jumps\", \"over\"], [\"the\", \"lazy\", \"dog\"]]\n",
    "    co_occurrence_matrix, word_to_index, index_to_word = create_co_occurrence_matrix(corpus, contexts)\n",
    "    print(co_occurrence_matrix)\n",
    "    print(word_to_index)\n",
    "    print(index_to_word)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    distinct_words = sorted(list(set(corpus)))\n",
    "    num_words = len(distinct_words)\n",
    "    num_contexts = len(contexts)\n",
    "   \n",
    "    print(f\"Number of distinct words: {num_words}\")\n",
    "    print(f\"Number of words in corpus: {len(corpus)}\")\n",
    "\n",
    "    # Create word to index and index to word mappings\n",
    "    word_to_index = {word: index for index, word in enumerate(distinct_words)}\n",
    "    index_to_word = {index: word for index, word in enumerate(distinct_words)}\n",
    "    \n",
    "\n",
    "    # only words with minimum 10 appearances in corpus are inculeded in the co-occurrence matrix\n",
    "    word_counts = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "    for i, word in enumerate(distinct_words):\n",
    "        if word_counts[word] >= min_frequency:\n",
    "            word_to_index[word] = i \n",
    "            index_to_word[i] = word\n",
    "        else:\n",
    "            del word_to_index[word]\n",
    "            del index_to_word[i]\n",
    "    # Rearrange the word to index and index to word mappings\n",
    "    word_to_index = {word: index for index, word in enumerate(word_to_index.keys())}\n",
    "    index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "    num_words = len(word_to_index)\n",
    "    print(f\"Number of distinct words with at least {min_frequency} appearances: {num_words}\")\n",
    "\n",
    "    # Initialize co-occurrence matrix\n",
    "    co_occurrence_matrix = np.zeros((num_words, num_contexts))\n",
    "\n",
    "    # Iterate over each context\n",
    "    for context_index, context in enumerate(contexts):\n",
    "        # Iterate over each word in the index that is in the context\n",
    "\n",
    "        for word in context:\n",
    "            if word not in word_to_index:\n",
    "                continue\n",
    "            word_index = word_to_index[word]\n",
    "            co_occurrence_matrix[word_index, context_index] += 1\n",
    "    \n",
    "    return co_occurrence_matrix, word_to_index, index_to_word\n",
    "\n",
    "# Example usage:\n",
    "# corpus = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "# contexts = [[\"the\", \"quick\", \"brown\"], [\"fox\", \"jumps\", \"over\"], [\"the\", \"lazy\", \"dog\"]]\n",
    "\n",
    "# co_occurrence_matrix, word_to_index, index_to_word = create_co_occurrence_matrix(corpus[:10000], contexts)\n",
    "# print(co_occurrence_matrix)\n",
    "# # print(word_to_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep co-occurence matrix for svd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paper:\n",
    "Prior to SVD, values in the matrix were log-transformed. The logs associated with each word were then divided by that word’s entropy (H) in the corpus: $$H  = \\sum_{c} p_c \\log{(p_c)}$$  where $c$ indexes the different contexts in which the word appears and $p_c$ denotes the word’s frequency in the context divided by its total frequency in the corpus. \n",
    "These standard transformations were performed to reduce the influence of very high-frequency function words whose patterns of occurrence were not relevant in generating the semantic space (Landauer & Dumais, 1997)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_co_occurrence(co_occurrence_matrix, word_to_index):\n",
    "    # Suppress warnings for dividing by zero\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    # print(f'Co-occurrence matrix: {co_occurrence_matrix}')\n",
    "    # print(f'Co-occurrence matrix shape: {co_occurrence_matrix.shape}')\n",
    "    # print(f'Word to index mapping length: {len(word_to_index)}')\n",
    "    # print(f'Index to word mapping length: {len(index_to_word)}')\n",
    "\n",
    "    # print(f'total word in first context: {len(contexts[0])} and total via matrix: {np.sum(co_occurrence_matrix[:,0])}')\n",
    "\n",
    "    # dividing frequency of occurrence of each word in each context by the total frequency of occurrence of the word in all contexts\n",
    "    prob = np.divide(co_occurrence_matrix, np.sum(co_occurrence_matrix, axis=1).reshape(-1, 1))\n",
    "    # print(f'Probability of each word in all contexts: {prob}')\n",
    "    # print(f'Probability of each word in all contexts shape: {prob.shape}')\n",
    "\n",
    "    # log transformation of the prob variable, if the each entry is not 0, else replace with 0\n",
    "    log_prob = np.where(prob > 0, np.log(prob), prob)\n",
    "    # log_prob = np.nan_to_num(log_prob)\n",
    "    # print(f'Log transformation of the probability matrix: {log_prob}')\n",
    "    # print(f'Log transformation of the probability matrix shape: {log_prob.shape}')\n",
    "\n",
    "    # applying pointwise multiplication of the log transformed matrix and the prob matrix\n",
    "    per_context_entropy_matrix = np.multiply(log_prob, prob)\n",
    "    # print(f'Pointwise multiplication of the log transformed matrix and the prob matrix: {per_context_entropy_matrix}')\n",
    "    # print(f'Pointwise multiplication of the log transformed matrix and the prob matrix shape: {per_context_entropy_matrix.shape}')\n",
    "\n",
    "    # adding all pointwise multiplication values for each word\n",
    "    word_entropies = np.sum(per_context_entropy_matrix, axis=1)\n",
    "    # print(f'Entropy of each word in within the whole corpus: {word_entropies}')\n",
    "    # print(f'Entropy of each word in within the whole corpus shape: {word_entropies.shape}')\n",
    "\n",
    "    #reshaping word entropies to match the shape of the co-occurrence matrix\n",
    "    word_entropies = np.reshape(word_entropies, (len(word_entropies), 1))\n",
    "    # print(f'Entropy of each word in within the whole corpus reshaped: {word_entropies}')\n",
    "    # print(f'Entropy of each word in within the whole corpus reshaped shape: {word_entropies.shape}')\n",
    "\n",
    "    # log transform the co-occurrence matrix but if the value is 0, replace with 0\n",
    "    co_occurrence_matrix = -np.where(co_occurrence_matrix > 0, np.log(co_occurrence_matrix), co_occurrence_matrix)\n",
    "    co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n",
    "    # print(f'Log transformation of the co-occurrence matrix: {co_occurrence_matrix}')\n",
    "\n",
    "    # dividing the occurrence of each word in each context by the entropy of the word i.e. divide each number in each row by the number in word_entropy[row]\n",
    "    co_occurrence_matrix = np.divide(co_occurrence_matrix, word_entropies)\n",
    "    \n",
    "    # print(f'Co-occurrence matrix(first two entries) : {co_occurrence_matrix[:2]}')  \n",
    "    # print(f'Co-occurrence matrix shape: {co_occurrence_matrix.shape}') # (number of words, number of contexts)\n",
    "\n",
    "    # Remove infinities and NaNs\n",
    "    co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n",
    "    \n",
    "    # Reset warnings filter to default\n",
    "    warnings.filterwarnings(\"default\", category=RuntimeWarning)\n",
    "\n",
    "    return co_occurrence_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD def: svd(co-occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd(co_occurrence_matrix):\n",
    "    '''\n",
    "    Singular Value Decomposition\n",
    "    - `u` represents the left singular vectors and contains information about the relationships between the rows of the original matrix. \n",
    "    The rows of the co-occurrence matrix represent the words in the corpus, so the left singular vectors contain information about the relationships between the words.\n",
    "\n",
    "    - `s` represents the singular values and contains information about the importance of each singular vector.\n",
    "\n",
    "    - `v` represents the right singular vectors and contains information about the relationships between the columns of the original matrix.\n",
    "    The columns of the co-occurrence matrix represent the contexts in which the words appear, so the right singular vectors contain information about the relationships between the contexts.\n",
    "    '''\n",
    "    # Singular Value Decomposition \n",
    "    u, s, v = np.linalg.svd(co_occurrence_matrix, full_matrices=False)\n",
    "    v = np.transpose(v)\n",
    "    print(f'U shape: {u.shape}')\n",
    "    print(f'S shape: {s.shape}')\n",
    "    print(f'V shape: {v.shape}')\n",
    "    return u, s, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA def:  perform_lsa(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lsa(corpus, context_length=1000, min_frequency = 5):\n",
    "    \"\"\"\n",
    "    Perform Latent Semantic Analysis on the given corpus.\n",
    "\n",
    "    Parameters:\n",
    "    - corpus (list): A list of words representing the corpus.\n",
    "    - context_length (int): The length of each context.\n",
    "    - min_frequency (int): The minimum number of times a word must appear in the corpus to be included in the co-occurrence matrix.\n",
    "\n",
    "    Returns:\n",
    "    - word_vectors (dict): A dictionary mapping words to their corresponding vectors.\n",
    "    - context_vectors (numpy.ndarray): A matrix where the rows represent contexts and the columns represent the latent dimensions.\n",
    "    - word_to_index (dict): A dictionary mapping words to their corresponding indices in the matrix.\n",
    "    - contexts (list): A list of lists, where each inner list represents a context of length context_length.\n",
    "\n",
    "    \"\"\"\n",
    "    contexts = context_splitting(corpus, context_length)\n",
    "    co_occurrence_matrix, word_to_index, index_to_word = create_co_occurrence_matrix(corpus,contexts, min_frequency = min_frequency)\n",
    "\n",
    "\n",
    "    # print(f'Co occurance matrix shape: {co_occurrence_matrix.shape}')\n",
    "    # print(f'Word to index mapping length: {len(word_to_index)}')\n",
    "    # print(f'Index to word mapping length: {len(index_to_word)}')\n",
    "\n",
    "    co_occurrence_matrix = prepare_co_occurrence(co_occurrence_matrix, word_to_index)\n",
    "\n",
    "    # Perform singular value decomposition\n",
    "    u, s, v = svd(co_occurrence_matrix)\n",
    "\n",
    "    # word_vectors = {word: u[i, :300] for i, word in enumerate(index_to_word)} # 300 dimensions is supposedly optimal according to the article.\n",
    "    word_vectors = {word: u[i, :] for i, word in enumerate(index_to_word)} # I'm doing all values unlike the article, because I'm not sure if the 300 dimensions are optimal.\n",
    "    # context_vectors = vt[:300, :]\n",
    "    context_vectors = v[:, :]\n",
    "\n",
    "    print(f'Word vectors length: {len(word_vectors)}')\n",
    "    print(f'Context vectors length: {len(context_vectors)}')\n",
    "    \n",
    "    return word_vectors, context_vectors, word_to_index, contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(word1, word2, word_vectors, word_to_index):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two words.\n",
    "    \n",
    "    Args:\n",
    "        word1 (str): The first word.\n",
    "        word2 (str): The second word.\n",
    "        word_vectors (dict): A dictionary mapping words to LSA vectors.\n",
    "    \n",
    "    Returns:\n",
    "        cosine_similarity (float): The cosine similarity between the two words.\n",
    "    \"\"\"\n",
    "    if word1 not in word_to_index or word2 not in word_to_index:\n",
    "        return print(\"One or both of the words are not in the index\")\n",
    "    \n",
    "    vector1 = word_vectors[word_to_index[word1]]\n",
    "    vector2 = word_vectors[word_to_index[word2]]\n",
    "    \n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vector1, vector2)    \n",
    "\n",
    "    # Calculate magnitudes\n",
    "    magnitude1 = np.sqrt(np.sum(vector1**2))\n",
    "    magnitude2 = np.sqrt(np.sum(vector2**2))\n",
    "\n",
    "    # Prevent division by zero\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return \n",
    "\n",
    "    cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    return cosine_similarity\n",
    "\n",
    "# word1= \"man\"\n",
    "# word2= \"woman\"\n",
    "# print(f'Cosine similarity between {word1} and {word2}: {get_cosine_similarity(word1, word2, word_vectors, word_to_index)}')\n",
    "\n",
    "# word1= \"boy\"\n",
    "# word2= \"girl\"\n",
    "# print(f'Cosine similarity between {word1} and {word2}: {get_cosine_similarity(word1, word2, word_vectors, word_to_index)}')\n",
    "\n",
    "\n",
    "def cosine_similarity_matrix(matrix):\n",
    "  \"\"\"\n",
    "  Calculates the cosine similarity matrix for a given matrix.\n",
    "\n",
    "  Args:\n",
    "      matrix: A 2D NumPy array representing the matrix of vectors.\n",
    "\n",
    "  Returns:\n",
    "      A 2D NumPy array containing the cosine similarity between all pairs of vectors in the input matrix.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate pairwise dot products\n",
    "  dot_products = np.matmul(matrix, matrix.T)\n",
    "\n",
    "  # Calculate vector magnitudes\n",
    "  magnitudes = np.linalg.norm(matrix, axis=1)[:, np.newaxis] * np.linalg.norm(matrix, axis=1)\n",
    "\n",
    "  # Prevent division by zero (set diagonal to 1 for self-similarity)\n",
    "  np.fill_diagonal(magnitudes, 1)\n",
    "\n",
    "  # Cosine similarity formula (avoiding division by zero)\n",
    "  cosine_similarities = dot_products / magnitudes\n",
    "\n",
    "  return cosine_similarities\n",
    "\n",
    "def add_word_probabilities(save_path, corpus, save=False):\n",
    "    # read all the words in the csv file\n",
    "    df = pd.read_csv(save_path)\n",
    "    print(df.head())\n",
    "\n",
    "    # calculate the probability of the word appearing in the corpus\n",
    "    word_counts = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "    total_words = len(corpus)\n",
    "    df['Word Probability'] = df['Word'].apply(lambda x: word_counts[x]/total_words)\n",
    "    print(df.head())\n",
    "\n",
    "    if save:\n",
    "        df.to_csv(save_path, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemD def:  calculate_semd(word, word_vectors, context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_semd(word, word_vectors, context_vectors, word_to_index,contexts, min_contexts=2):\n",
    "    \"\"\"\n",
    "    Calculate the semantic diversity (SemD) of the given word.\n",
    "    \n",
    "    For a given word, take all contexts in which it appeared and calculate the cosine between each pair of contexts in the set. \n",
    "    Then take the mean of these cosines to represent the average similarity between any two contexts containing the word. \n",
    "    Then take the negative log of this value to represent the semantic diversity of the word. \n",
    "    \n",
    "    The higher the value, the more diverse the contexts in which the word appears.\n",
    "    If the word appears in fewer than 4 contexts, return 0.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word for which to calculate SemD.\n",
    "        word_vectors (dict): A dictionary mapping words to LSA vectors.\n",
    "        context_vectors (list): A list of LSA vectors, one for each context.\n",
    "    \n",
    "    Returns:\n",
    "        semd (float): The semantic diversity value for the word.\n",
    "    \"\"\"\n",
    "    # Suppress warnings for dividing by zero\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    # Find all contexts indices containing the word \n",
    "    \n",
    "    # context_indices = [i for i, context in enumerate(contexts) if word in context]\n",
    "    context_indices = []\n",
    "    for i, context in enumerate(contexts):\n",
    "        if word in context:\n",
    "            context_indices.append(i)\n",
    "\n",
    "    # If the word is not in any context, or if the word is in fewer than 5 contexts, return None  \n",
    "    #if not context_indices or len(context_indices) < min_contexts:\n",
    "    #    semd = 0\n",
    "    #    return semd\n",
    "\n",
    "    # create a matrix of all the contexts containing the word and transpose it\n",
    "    word_contexts = np.array([context_vectors[i] for i in context_indices])      \n",
    "    \n",
    "    # Based on the paper, we should only consider a random sample of 2000 contexts.\n",
    "    # if len(word_contexts) > 2000:\n",
    "    #     word_contexts = np.random.choice(word_contexts, size=2000, replace=False)\n",
    "\n",
    "    # Calculate the average cosine similarity between the contexts\n",
    "    context_similarities = cosine_similarity_matrix(word_contexts)\n",
    "    mean_similarity = np.mean(context_similarities)\n",
    "\n",
    "    semd = mean_similarity\n",
    "    # Calculate the SemD value\n",
    "    semd = -np.log(mean_similarity)\n",
    "\n",
    "    # Remove infinities and NaNs\n",
    "    semd = np.nan_to_num(semd)\n",
    "\n",
    "    # Reset warnings filter to default\n",
    "    warnings.filterwarnings(\"default\", category=RuntimeWarning)\n",
    "    \n",
    "    return semd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_semD(df, corpus_name,context_length,min_frequency,min_contexts, save=False):\n",
    "\n",
    "    # Plot the semD values using plotly\n",
    "    fig = px.bar(df, x='Word', y='SemD', title=f'SemD Values for Words in the {corpus_name} Corpus')\n",
    "\n",
    "    # add color to the plot\n",
    "    fig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)',marker_line_width=1.5, opacity=0.6)\n",
    "    fig.show()\n",
    "    \n",
    "    if save:\n",
    "        # save figure\n",
    "        fig.write_html(f\"../Code/{corpus_name}_semD_Basic_lowercase_{context_length}word_contexts_min{min_frequency}freq_min{min_contexts}contexts.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_semd_wrapper(word, word_vectors, context_vectors, word_to_index, contexts, min_contexts):\n",
    "    '''\n",
    "    Wrapper function for calculating SemD values in parallel.\n",
    "    '''\n",
    "    semd_value = calculate_semd(word, word_vectors, context_vectors, word_to_index, contexts, min_contexts)\n",
    "    return word, semd_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_rank_correlation(df):\n",
    "    # conduct spearman rank correlation test of the word and their word entropy\n",
    "    correlation = scipy.stats.spearmanr(df['SemD'], df['Word Probability'])\n",
    "    print(f'Spearman Rank Correlation: {correlation}')\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrapping all definitions in a master definition\n",
    "corpus_semd\n",
    "It's not working correctly right now. Using the internal code outside of the funiton to generate the plots correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_semd(directory, context_length, min_frequency, min_contexts, save=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Calculate SemD values for all words in the corpus.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    - directory (str): The directory containing the corpus files.\n",
    "    - context_length (int): The length of each context in words.\n",
    "    - min_frequency (int): The minimum number of times a word must appear in the corpus to be included in the co-occurrence matrix.\n",
    "    - min_contexts (int): The minimum number of contexts in which a word must appear to calculate its SemD value.\n",
    "    - save (bool): Whether to save the SemD values to a CSV file.\n",
    "    - save_path (str): The directory in which to save the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): A DataFrame containing the SemD values and word probabilities for all qualifying words in the corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    corpus_name = directory.split('/')[-2]\n",
    "    corpus = load_corpus(directory)\n",
    "\n",
    "    # Performing LSA for the whole corpus\n",
    "    word_vectors, context_vectors, word_to_index, contexts = perform_lsa(corpus, context_length=context_length, min_frequency=min_frequency)\n",
    "    print(f'Context vectors shape: {context_vectors.shape}')\n",
    "\n",
    "    # # Calculate semD values for all words in the index using the Pool method\n",
    "    # with Pool() as p:\n",
    "    #     semD_values_list = p.map(calculate_semd_wrapper, [(word, word_vectors, context_vectors, word_to_index, contexts, min_contexts) for word in word_to_index])\n",
    "    # semD_values = dict(semD_values_list)\n",
    "\n",
    "    # Calculate semD values for all words in the index and plot them with the corresponding word label\n",
    "    semD_values = {word: calculate_semd(word, word_vectors, context_vectors, word_to_index, contexts, min_contexts) for word in word_to_index}  \n",
    "\n",
    "\n",
    "    # Sort semD values in descending order of their values\n",
    "    semD_values = {k: v for k, v in sorted(semD_values.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    # Create a DataFrame from the semD values\n",
    "    df = pd.DataFrame(list(semD_values.items()), columns=['Word', 'SemD'])\n",
    "\n",
    "    plot_semD(df, corpus_name, context_length, min_frequency, min_contexts, save=save)\n",
    "\n",
    "    # calculate the probability of the word appearing in the corpus\n",
    "    word_counts = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "    total_words = len(corpus)\n",
    "    df['Word Probability'] = df['Word'].apply(lambda x: word_counts[x]/total_words)\n",
    "    print(df.head())\n",
    "\n",
    "    if save:\n",
    "        # Save the DataFrame as a CSV file\n",
    "        file_name = f'{corpus_name}_semD_Basic_lowercase_{context_length}word_contexts_min_freq_{min_frequency}_min_contexts_{min_contexts}.csv'\n",
    "        save_path = os.path.join(save_path, file_name)\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f'SemD values saved to {save_path}')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating LSA\n",
    "and getting word vectors context vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating SemD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 1000\n",
    "min_frequency = 50\n",
    "min_contexts = 5\n",
    "\n",
    "# directory_names = ['Shakespeare', 'Oscar Wilde', 'Mark Twain', 'Rudyard Kipling', 'G K Chesterton', 'Arthur Conan Doyle', 'Charles Dickens','Tolstoy']\n",
    "directory_names = ['Shakespeare']\n",
    "for directory_name in directory_names:\n",
    "    directory = f'../Text data/{directory_name}/'\n",
    "    df = corpus_semd(directory, context_length, min_frequency, min_contexts)\n",
    "spearman_rank_correlation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 1000\n",
    "min_frequency = 90\n",
    "min_contexts = 5\n",
    "\n",
    "# directory_names = ['Shakespeare', 'Oscar Wilde', 'Mark Twain', 'Rudyard Kipling', 'G K Chesterton', 'Arthur Conan Doyle', 'Charles Dickens','Tolstoy']\n",
    "directory_names = ['Shakespeare']\n",
    "for directory_name in directory_names:\n",
    "    directory = f'../Text data/{directory_name}/'\n",
    "    df = corpus_semd(directory, context_length, min_frequency, min_contexts)\n",
    "spearman_rank_correlation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #playing with multiprocessing\n",
    "\n",
    "# context_length = 1000\n",
    "# min_frequency = 1000\n",
    "# min_contexts = 50\n",
    "\n",
    "# directory_name = 'Text Data'\n",
    "# directory = f'../{directory_name}/'\n",
    "# # semD_df= corpus_semd(directory, context_length=context_length, min_frequency=min_frequency, min_contexts=min_contexts)\n",
    "\n",
    "# corpus_name = directory.split('/')[-2]\n",
    "# corpus = load_corpus(directory)\n",
    "\n",
    "# # Performing LSA for the whole corpus\n",
    "# word_vectors, context_vectors, word_to_index, contexts = perform_lsa(corpus, context_length = context_length, min_frequency=min_frequency)\n",
    "# print(f'Context vectors shape: {context_vectors.shape}')\n",
    "\n",
    "# def calculate_semd_wrapper(args):\n",
    "#     word, word_vectors, context_vectors, word_to_index, min_contexts = args\n",
    "#     semd_value = calculate_semd(word, word_vectors, context_vectors, word_to_index, min_contexts)\n",
    "#     return word, semd_value\n",
    "\n",
    "# with Pool() as p:\n",
    "#     semD_values_list = p.map(calculate_semd_wrapper, [(word, word_vectors, context_vectors, word_to_index, min_contexts) for word in word_to_index])\n",
    "\n",
    "# semD_values = dict(semD_values_list)\n",
    "# # # Calculate semD values for all words in the index and plot them with the corresponding word label\n",
    "# # semD_values = {word: calculate_semd(word, word_vectors, context_vectors, word_to_index, min_contexts) for word in word_to_index}\n",
    "\n",
    "# # Sort semD values in descending order of their values\n",
    "# semD_values = {k: v for k, v in sorted(semD_values.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "\n",
    "# # Create a DataFrame from the semD values\n",
    "# df = pd.DataFrame(list(semD_values.items()), columns=['Word', 'SemD'])\n",
    "\n",
    "# save_path = f'../Code/{corpus_name}_semD_Basic_lowercase_{context_length}word_contexts_min_freq_{min_frequency}_min_contexts_{min_contexts}.csv'\n",
    "# df.to_csv(save_path, index=False)\n",
    "\n",
    "# # Plot the semD values using plotly\n",
    "# fig = px.bar(df, x='Word', y='SemD', title=f'SemD Values for Words in the {corpus_name} Corpus')\n",
    "\n",
    "# # add color to the plot\n",
    "# fig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)',marker_line_width=1.5, opacity=0.6)\n",
    "# fig.show()\n",
    "\n",
    "# # save figure\n",
    "# fig.write_html(f\"../Code/{corpus_name}_semD_Basic_lowercase_{context_length}word_contexts_min{min_frequency}freq_min{min_contexts}contexts.html\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = 'Text data_semD_Basic_lowercase_1000word_contexts_min_freq_1000_min_contexts_50.csv'\n",
    "# def add_word_probabilities(save_path, corpus, save=False):\n",
    "#     # read all the words in the csv file\n",
    "#     df = pd.read_csv(save_path)\n",
    "#     print(df.head())\n",
    "\n",
    "#     # calculate the probability of the word appearing in the corpus\n",
    "#     word_counts = defaultdict(int)\n",
    "#     for word in corpus:\n",
    "#         word_counts[word] += 1\n",
    "\n",
    "#     total_words = len(corpus)\n",
    "#     df['Word Probability'] = df['Word'].apply(lambda x: word_counts[x]/total_words)\n",
    "#     print(df.head())\n",
    "\n",
    "#     if save:\n",
    "#         df.to_csv(save_path, index=False)\n",
    "#     return df\n",
    "\n",
    "# df = add_word_probabilities(save_path, corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 1000\n",
    "min_frequency = 500\n",
    "min_contexts = 5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoftComputing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
