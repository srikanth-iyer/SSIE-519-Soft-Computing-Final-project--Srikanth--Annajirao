{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/siyer/SSIE-519-Soft-Computing-Final-project--Srikanth--Annajirao/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from scipy.spatial import distance\n",
    "import scipy\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "from scipy.spatial import distance\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = []\n",
    "\n",
    "def load_corpus(directory):\n",
    "    \"\"\"\n",
    "    Load the corpus from the directory and return a list of all lemma words\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): The directory path where the XML files are located.\n",
    "\n",
    "    Returns:\n",
    "    - corpus (list): A list of lemma words based on the input after hw=.\n",
    "\n",
    "    Example:\n",
    "    >>> directory = '../Other_text_data/bnc/2553/Texts/fic'\n",
    "    >>> corpus = load_corpus(directory)\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = []\n",
    "    file_paths = glob.glob(os.path.join(directory, '**/*.xml'), recursive=True)\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for word in root.iter('w'):\n",
    "            lemma = word.get('hw')\n",
    "            corpus.append(lemma)\n",
    "\n",
    "    print(f'Length of Corpus: {len(corpus)}')\n",
    "    print(f'First 20 words in the corpus: {corpus[:20]}')\n",
    "    return corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## context splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_splitting(corpus, context_length=1000):\n",
    "    \"\"\"\n",
    "    Split the corpus into contexts of length context_length\n",
    "\n",
    "    Parameters:\n",
    "    - corpus (list): A list of words\n",
    "    - context_length (int): The length of each context\n",
    "\n",
    "    Returns:\n",
    "    - contexts (list): A list of contexts, where each context is a list of words\n",
    "\n",
    "    Example:\n",
    "    >>> corpus = ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "    >>> context_length = 2\n",
    "    >>> context_splitting(corpus, context_length)\n",
    "    [['a', 'b'], ['c', 'd'], ['e', 'f']]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    current_context = []\n",
    "    contexts = []\n",
    "\n",
    "    for word in corpus:\n",
    "        current_context.append(word)\n",
    "        if len(current_context) == context_length:\n",
    "            contexts.append(current_context)\n",
    "            current_context = []\n",
    "\n",
    "    if current_context:\n",
    "        contexts.append(current_context)\n",
    "    \n",
    "    return contexts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurence matrix def: create_co_occurrence_matrix(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_occurrence_matrix(corpus, contexts, min_frequency = 5):\n",
    "    '''\n",
    "    Create a co-occurrence matrix based on the given corpus and contexts.\n",
    "\n",
    "    Parameters:\n",
    "    - corpus (list): A list of words representing the corpus.\n",
    "    - contexts (list): A list of lists, where each inner list represents a context of default length 1000 words unless specified otherwise.\n",
    "    - min_frequency (int): The minimum number of times a word must appear in the corpus to be included in the co-occurrence matrix.\n",
    "\n",
    "    Returns:\n",
    "    - co_occurrence_matrix (numpy.ndarray): The rows represent words and the columns represent contexts. The value at (i, j) represents the number of times word i appears in context j.\n",
    "\n",
    "    - word_to_index (dict): A dictionary mapping words to their corresponding indices in the matrix.\n",
    "    \n",
    "    - index_to_word (dict): A dictionary mapping indices to their corresponding words in the matrix.\n",
    "\n",
    "    Example usage:\n",
    "    corpus = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "    contexts = [[\"the\", \"quick\", \"brown\"], [\"fox\", \"jumps\", \"over\"], [\"the\", \"lazy\", \"dog\"]]\n",
    "    co_occurrence_matrix, word_to_index, index_to_word = create_co_occurrence_matrix(corpus, contexts)\n",
    "    print(co_occurrence_matrix)\n",
    "    print(word_to_index)\n",
    "    print(index_to_word)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    distinct_words = sorted(list(set(corpus)))\n",
    "    num_words = len(distinct_words)\n",
    "    num_contexts = len(contexts)\n",
    "   \n",
    "    print(f\"Number of distinct words: {num_words}\")\n",
    "    print(f\"Number of words in corpus: {len(corpus)}\")\n",
    "\n",
    "    # Create word to index and index to word mappings\n",
    "    word_to_index = {word: index for index, word in enumerate(distinct_words)}\n",
    "    index_to_word = {index: word for index, word in enumerate(distinct_words)}\n",
    "    \n",
    "\n",
    "    # only words with minimum 10 appearances in corpus are inculeded in the co-occurrence matrix\n",
    "    word_counts = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "    for i, word in enumerate(distinct_words):\n",
    "        if word_counts[word] >= min_frequency:\n",
    "            word_to_index[word] = i \n",
    "            index_to_word[i] = word\n",
    "        else:\n",
    "            del word_to_index[word]\n",
    "            del index_to_word[i]\n",
    "    # Rearrange the word to index and index to word mappings\n",
    "    word_to_index = {word: index for index, word in enumerate(word_to_index.keys())}\n",
    "    index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "    num_words = len(word_to_index)\n",
    "    print(f\"Number of distinct words with at least {min_frequency} appearances: {num_words}\")\n",
    "\n",
    "    # Initialize co-occurrence matrix\n",
    "    co_occurrence_matrix = np.zeros((num_words, num_contexts))\n",
    "\n",
    "    # Iterate over each context\n",
    "    for context_index, context in enumerate(contexts):\n",
    "        # Iterate over each word in the index that is in the context\n",
    "\n",
    "        for word in context:\n",
    "            if word not in word_to_index:\n",
    "                continue\n",
    "            word_index = word_to_index[word]\n",
    "            co_occurrence_matrix[word_index, context_index] += 1\n",
    "    \n",
    "    return co_occurrence_matrix, word_to_index, index_to_word\n",
    "\n",
    "# Example usage:\n",
    "# corpus = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "# contexts = [[\"the\", \"quick\", \"brown\"], [\"fox\", \"jumps\", \"over\"], [\"the\", \"lazy\", \"dog\"]]\n",
    "\n",
    "# co_occurrence_matrix, word_to_index, index_to_word = create_co_occurrence_matrix(corpus[:10000], contexts)\n",
    "# print(co_occurrence_matrix)\n",
    "# # print(word_to_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep co-occurence matrix for svd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paper:\n",
    "Prior to SVD, values in the matrix were log-transformed. The logs associated with each word were then divided by that word’s entropy (H) in the corpus: $$H  = \\sum_{c} p_c \\log{(p_c)}$$  where $c$ indexes the different contexts in which the word appears and $p_c$ denotes the word’s frequency in the context divided by its total frequency in the corpus. \n",
    "These standard transformations were performed to reduce the influence of very high-frequency function words whose patterns of occurrence were not relevant in generating the semantic space (Landauer & Dumais, 1997)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_co_occurrence(co_occurrence_matrix, word_to_index):\n",
    "    # Suppress warnings for dividing by zero\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    # print(f'Co-occurrence matrix: {co_occurrence_matrix}')\n",
    "    # print(f'Co-occurrence matrix shape: {co_occurrence_matrix.shape}')\n",
    "    # print(f'Word to index mapping length: {len(word_to_index)}')\n",
    "    # print(f'Index to word mapping length: {len(index_to_word)}')\n",
    "\n",
    "    # print(f'total word in first context: {len(contexts[0])} and total via matrix: {np.sum(co_occurrence_matrix[:,0])}')\n",
    "\n",
    "    # dividing frequency of occurrence of each word in each context by the total frequency of occurrence of the word in all contexts\n",
    "    prob = np.divide(co_occurrence_matrix, np.sum(co_occurrence_matrix, axis=1).reshape(-1, 1))\n",
    "    # print(f'Probability of each word in all contexts: {prob}')\n",
    "    # print(f'Probability of each word in all contexts shape: {prob.shape}')\n",
    "\n",
    "    # log transformation of the prob variable, if the each entry is not 0, else replace with 0\n",
    "    log_prob = np.where(prob > 0, np.log(prob), prob)\n",
    "    # log_prob = np.nan_to_num(log_prob)\n",
    "    # print(f'Log transformation of the probability matrix: {log_prob}')\n",
    "    # print(f'Log transformation of the probability matrix shape: {log_prob.shape}')\n",
    "\n",
    "    # applying pointwise multiplication of the log transformed matrix and the prob matrix\n",
    "    per_context_entropy_matrix = np.multiply(log_prob, prob)\n",
    "    # print(f'Pointwise multiplication of the log transformed matrix and the prob matrix: {per_context_entropy_matrix}')\n",
    "    # print(f'Pointwise multiplication of the log transformed matrix and the prob matrix shape: {per_context_entropy_matrix.shape}')\n",
    "\n",
    "    # adding all pointwise multiplication values for each word\n",
    "    word_entropies = np.sum(per_context_entropy_matrix, axis=1)\n",
    "    # print(f'Entropy of each word in within the whole corpus: {word_entropies}')\n",
    "    # print(f'Entropy of each word in within the whole corpus shape: {word_entropies.shape}')\n",
    "\n",
    "    #reshaping word entropies to match the shape of the co-occurrence matrix\n",
    "    word_entropies = np.reshape(word_entropies, (len(word_entropies), 1))\n",
    "    # print(f'Entropy of each word in within the whole corpus reshaped: {word_entropies}')\n",
    "    # print(f'Entropy of each word in within the whole corpus reshaped shape: {word_entropies.shape}')\n",
    "\n",
    "    # log transform the co-occurrence matrix but if the value is 0, replace with 0\n",
    "    co_occurrence_matrix = -np.where(co_occurrence_matrix > 0, np.log(co_occurrence_matrix), co_occurrence_matrix)\n",
    "    co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n",
    "    # print(f'Log transformation of the co-occurrence matrix: {co_occurrence_matrix}')\n",
    "\n",
    "    # dividing the occurrence of each word in each context by the entropy of the word i.e. divide each number in each row by the number in word_entropy[row]\n",
    "    co_occurrence_matrix = np.divide(co_occurrence_matrix, word_entropies)\n",
    "    \n",
    "    # print(f'Co-occurrence matrix(first two entries) : {co_occurrence_matrix[:2]}')  \n",
    "    # print(f'Co-occurrence matrix shape: {co_occurrence_matrix.shape}') # (number of words, number of contexts)\n",
    "\n",
    "    # Remove infinities and NaNs\n",
    "    co_occurrence_matrix = np.nan_to_num(co_occurrence_matrix)\n",
    "    \n",
    "    # Reset warnings filter to default\n",
    "    warnings.filterwarnings(\"default\", category=RuntimeWarning)\n",
    "\n",
    "    return co_occurrence_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD def: svd(co-occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd(co_occurrence_matrix):\n",
    "    '''\n",
    "    Singular Value Decomposition\n",
    "    - `u` represents the left singular vectors and contains information about the relationships between the rows of the original matrix. \n",
    "    The rows of the co-occurrence matrix represent the words in the corpus, so the left singular vectors contain information about the relationships between the words.\n",
    "\n",
    "    - `s` represents the singular values and contains information about the importance of each singular vector.\n",
    "\n",
    "    - `v` represents the right singular vectors and contains information about the relationships between the columns of the original matrix.\n",
    "    The columns of the co-occurrence matrix represent the contexts in which the words appear, so the right singular vectors contain information about the relationships between the contexts.\n",
    "    '''\n",
    "    # Singular Value Decomposition \n",
    "    u, s, v = np.linalg.svd(co_occurrence_matrix, full_matrices=False)\n",
    "    v = np.transpose(v)\n",
    "    print(f'U shape: {u.shape}')\n",
    "    print(f'S shape: {s.shape}')\n",
    "    print(f'V shape: {v.shape}')\n",
    "    return u, s, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA def:  perform_lsa(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lsa(corpus, context_length=1000, min_frequency = 5):\n",
    "    \"\"\"\n",
    "    Perform Latent Semantic Analysis on the given corpus.\n",
    "\n",
    "    Parameters:\n",
    "    - corpus (list): A list of words representing the corpus.\n",
    "    - context_length (int): The length of each context.\n",
    "    - min_frequency (int): The minimum number of times a word must appear in the corpus to be included in the co-occurrence matrix.\n",
    "\n",
    "    Returns:\n",
    "    - word_vectors (dict): A dictionary mapping words to their corresponding vectors.\n",
    "    - context_vectors (numpy.ndarray): A matrix where the rows represent contexts and the columns represent the latent dimensions.\n",
    "    - word_to_index (dict): A dictionary mapping words to their corresponding indices in the matrix.\n",
    "    - contexts (list): A list of lists, where each inner list represents a context of length context_length.\n",
    "\n",
    "    \"\"\"\n",
    "    contexts = context_splitting(corpus, context_length)\n",
    "    co_occurrence_matrix, word_to_index, index_to_word = create_co_occurrence_matrix(corpus,contexts, min_frequency = min_frequency)\n",
    "\n",
    "\n",
    "    # print(f'Co occurance matrix shape: {co_occurrence_matrix.shape}')\n",
    "    # print(f'Word to index mapping length: {len(word_to_index)}')\n",
    "    # print(f'Index to word mapping length: {len(index_to_word)}')\n",
    "\n",
    "    co_occurrence_matrix = prepare_co_occurrence(co_occurrence_matrix, word_to_index)\n",
    "\n",
    "    # Perform singular value decomposition\n",
    "    u, s, v = svd(co_occurrence_matrix)\n",
    "\n",
    "    # word_vectors = {word: u[i, :300] for i, word in enumerate(index_to_word)} # 300 dimensions is supposedly optimal according to the article.\n",
    "    word_vectors = {word: u[i, :] for i, word in enumerate(index_to_word)} # I'm doing all values unlike the article, because I'm not sure if the 300 dimensions are optimal.\n",
    "    # context_vectors = vt[:300, :]\n",
    "    context_vectors = v[:, :]\n",
    "\n",
    "    print(f'Word vectors length: {len(word_vectors)}')\n",
    "    print(f'Context vectors length: {len(context_vectors)}')\n",
    "    \n",
    "    return word_vectors, context_vectors, word_to_index, contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(word1, word2, word_vectors, word_to_index):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two words.\n",
    "    \n",
    "    Args:\n",
    "        word1 (str): The first word.\n",
    "        word2 (str): The second word.\n",
    "        word_vectors (dict): A dictionary mapping words to LSA vectors.\n",
    "    \n",
    "    Returns:\n",
    "        cosine_similarity (float): The cosine similarity between the two words.\n",
    "    \"\"\"\n",
    "    if word1 not in word_to_index or word2 not in word_to_index:\n",
    "        return print(\"One or both of the words are not in the index\")\n",
    "    \n",
    "    vector1 = word_vectors[word_to_index[word1]]\n",
    "    vector2 = word_vectors[word_to_index[word2]]\n",
    "    \n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vector1, vector2)    \n",
    "\n",
    "    # Calculate magnitudes\n",
    "    magnitude1 = np.sqrt(np.sum(vector1**2))\n",
    "    magnitude2 = np.sqrt(np.sum(vector2**2))\n",
    "\n",
    "    # Prevent division by zero\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return \n",
    "\n",
    "    cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    return cosine_similarity\n",
    "\n",
    "# word1= \"man\"\n",
    "# word2= \"woman\"\n",
    "# print(f'Cosine similarity between {word1} and {word2}: {get_cosine_similarity(word1, word2, word_vectors, word_to_index)}')\n",
    "\n",
    "# word1= \"boy\"\n",
    "# word2= \"girl\"\n",
    "# print(f'Cosine similarity between {word1} and {word2}: {get_cosine_similarity(word1, word2, word_vectors, word_to_index)}')\n",
    "\n",
    "\n",
    "def cosine_similarity_matrix(matrix):\n",
    "  \"\"\"\n",
    "  Calculates the cosine similarity matrix for a given matrix.\n",
    "\n",
    "  Args:\n",
    "      matrix: A 2D NumPy array representing the matrix of vectors.\n",
    "\n",
    "  Returns:\n",
    "      A 2D NumPy array containing the cosine similarity between all pairs of vectors in the input matrix.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate pairwise dot products\n",
    "  dot_products = np.matmul(matrix, matrix.T)\n",
    "\n",
    "  # Calculate vector magnitudes\n",
    "  magnitudes = np.linalg.norm(matrix, axis=1)[:, np.newaxis] * np.linalg.norm(matrix, axis=1)\n",
    "\n",
    "  # Prevent division by zero (set diagonal to 1 for self-similarity)\n",
    "  np.fill_diagonal(magnitudes, 1)\n",
    "\n",
    "  # Cosine similarity formula (avoiding division by zero)\n",
    "  cosine_similarities = dot_products / magnitudes\n",
    "\n",
    "  return cosine_similarities\n",
    "\n",
    "def add_word_probabilities(save_path, corpus, save=False):\n",
    "    # read all the words in the csv file\n",
    "    df = pd.read_csv(save_path)\n",
    "    print(df.head())\n",
    "\n",
    "    # calculate the probability of the word appearing in the corpus\n",
    "    word_counts = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "    total_words = len(corpus)\n",
    "    df['Word Probability'] = df['Word'].apply(lambda x: word_counts[x]/total_words)\n",
    "    print(df.head())\n",
    "\n",
    "    if save:\n",
    "        df.to_csv(save_path, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemD def:  calculate_semd(word, word_vectors, context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_semd(word, word_vectors, context_vectors, word_to_index,contexts, min_contexts=2):\n",
    "    \"\"\"\n",
    "    Calculate the semantic diversity (SemD) of the given word.\n",
    "    \n",
    "    For a given word, take all contexts in which it appeared and calculate the cosine between each pair of contexts in the set. \n",
    "    Then take the mean of these cosines to represent the average similarity between any two contexts containing the word. \n",
    "    Then take the negative log of this value to represent the semantic diversity of the word. \n",
    "    \n",
    "    The higher the value, the more diverse the contexts in which the word appears.\n",
    "    If the word appears in fewer than 4 contexts, return 0.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word for which to calculate SemD.\n",
    "        word_vectors (dict): A dictionary mapping words to LSA vectors.\n",
    "        context_vectors (list): A list of LSA vectors, one for each context.\n",
    "    \n",
    "    Returns:\n",
    "        semd (float): The semantic diversity value for the word.\n",
    "    \"\"\"\n",
    "    # Suppress warnings for dividing by zero\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    # Find all contexts indices containing the word \n",
    "    \n",
    "    # context_indices = [i for i, context in enumerate(contexts) if word in context]\n",
    "    context_indices = []\n",
    "    for i, context in enumerate(contexts):\n",
    "        if word in context:\n",
    "            context_indices.append(i)\n",
    "\n",
    "    # If the word is not in any context, or if the word is in fewer than 5 contexts, return None  \n",
    "    #if not context_indices or len(context_indices) < min_contexts:\n",
    "    #    semd = 0\n",
    "    #    return semd\n",
    "\n",
    "    # create a matrix of all the contexts containing the word and transpose it\n",
    "    word_contexts = np.array([context_vectors[i] for i in context_indices])      \n",
    "    \n",
    "    # Based on the paper, we should only consider a random sample of 2000 contexts.\n",
    "    # if len(word_contexts) > 2000:\n",
    "    #     word_contexts = np.random.choice(word_contexts, size=2000, replace=False)\n",
    "\n",
    "    # Calculate the average cosine similarity between the contexts\n",
    "    context_similarities = cosine_similarity_matrix(word_contexts)\n",
    "    mean_similarity = np.mean(context_similarities)\n",
    "\n",
    "    semd = mean_similarity\n",
    "    # Calculate the SemD value\n",
    "    semd = -np.log(mean_similarity)\n",
    "\n",
    "    # Remove infinities and NaNs\n",
    "    semd = np.nan_to_num(semd)\n",
    "\n",
    "    # Reset warnings filter to default\n",
    "    warnings.filterwarnings(\"default\", category=RuntimeWarning)\n",
    "    \n",
    "    return semd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_semD(df, corpus_name,context_length,min_frequency,min_contexts, save=False, save_path = None):\n",
    "\n",
    "    # Plot the semD values using plotly\n",
    "    fig = px.bar(df, x='Word', y='SemD', title=f'SemD Values for Words in the {corpus_name} Corpus')\n",
    "\n",
    "    # add color to the plot\n",
    "    fig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)',marker_line_width=1.5, opacity=0.6)\n",
    "    fig.show()\n",
    "    \n",
    "    if save:\n",
    "        # save figure\n",
    "        if save_path:\n",
    "            fig.write_html(f\"{save_path}/{corpus_name}_semD_Basic_lowercase_{context_length}word_contexts_min{min_frequency}freq_min{min_contexts}contexts.html\")\n",
    "        else:\n",
    "            fig.write_html(f\"../Code/{corpus_name}_semD_Basic_lowercase_{context_length}word_contexts_min{min_frequency}freq_min{min_contexts}contexts.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_semd_wrapper(args):\n",
    "    '''\n",
    "    Wrapper function for calculating SemD values in parallel.\n",
    "    '''\n",
    "    word, word_vectors, context_vectors, word_to_index, contexts, min_contexts = args\n",
    "    semd_value = calculate_semd(word, word_vectors, context_vectors, word_to_index, contexts, min_contexts)\n",
    "    return word, semd_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_rank_correlation(df):\n",
    "    # conduct spearman rank correlation test of the word and their word entropy\n",
    "    correlation = scipy.stats.spearmanr(df['SemD'], df['Word Probability'])\n",
    "    print(f'Spearman Rank Correlation: {correlation}')\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrapping all definitions in a master definition\n",
    "corpus_semd\n",
    "It's not working correctly right now. Using the internal code outside of the funiton to generate the plots correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_semd(directory, context_length, min_frequency, min_contexts, save=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Calculate SemD values for all words in the corpus.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    - directory (str): The directory containing the corpus files.\n",
    "    - context_length (int): The length of each context in words.\n",
    "    - min_frequency (int): The minimum number of times a word must appear in the corpus to be included in the co-occurrence matrix.\n",
    "    - min_contexts (int): The minimum number of contexts in which a word must appear to calculate its SemD value.\n",
    "    - save (bool): Whether to save the SemD values to a CSV file.\n",
    "    - save_path (str): The directory in which to save the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): A DataFrame containing the SemD values and word probabilities for all qualifying words in the corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    corpus_name = directory.split('/')[-2]\n",
    "    corpus = load_corpus(directory)\n",
    "\n",
    "    # Performing LSA for the whole corpus\n",
    "    word_vectors, context_vectors, word_to_index, contexts = perform_lsa(corpus, context_length=context_length, min_frequency=min_frequency)\n",
    "    print(f'Context vectors shape: {context_vectors.shape}')\n",
    "\n",
    "    # # Calculate semD values for all words in the index using the Pool method\n",
    "    # with Pool() as p:\n",
    "    #     semD_values_list = p.map(calculate_semd_wrapper, [(word, word_vectors, context_vectors, word_to_index, contexts, min_contexts) for word in word_to_index])\n",
    "    # semD_values = dict(semD_values_list)\n",
    "\n",
    "    # Calculate semD values for all words in the index and plot them with the corresponding word label\n",
    "    semD_values = {word: calculate_semd(word, word_vectors, context_vectors, word_to_index, contexts, min_contexts) for word in word_to_index}  \n",
    "\n",
    "\n",
    "    # Sort semD values in descending order of their values\n",
    "    semD_values = {k: v for k, v in sorted(semD_values.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    # Create a DataFrame from the semD values\n",
    "    df = pd.DataFrame(list(semD_values.items()), columns=['Word', 'SemD'])\n",
    "\n",
    "    plot_semD(df, corpus_name, context_length, min_frequency, min_contexts, save=save, save_path=save_path)\n",
    "\n",
    "    # calculate the probability of the word appearing in the corpus\n",
    "    word_counts = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "    total_words = len(corpus)\n",
    "    df['Word Probability'] = df['Word'].apply(lambda x: word_counts[x]/total_words)\n",
    "    print(df.head())\n",
    "\n",
    "    if save:\n",
    "        # Save the DataFrame as a CSV file\n",
    "        file_name = f'{corpus_name}_semD_Basic_lowercase_{context_length}word_contexts_min_freq_{min_frequency}_min_contexts_{min_contexts}.csv'\n",
    "        save_path = os.path.join(save_path, file_name)\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f'SemD values saved to {save_path}')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 13, 19, 26, 37, 51, 71, 98, 136, 189, 263, 365, 506, 702, 974, 1351, 1874, 2599, 3605, 4999]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "min_freq = np.logspace(np.log10(10), np.log10(5000), num=20).astype(int).tolist()\n",
    "print(min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "context_length = 1000\n",
    "\n",
    "min_frequency = [5000, 2500, 1250, 500, 200, 100, 50, 25, 10]\n",
    "min_contexts = 10\n",
    "directory = '../Other_text_data/bnc/2553/Texts/fic/'\n",
    "directory_name = directory.split('/')[-2]\n",
    "for min_frequency in min_frequency:\n",
    "    df = corpus_semd(directory, context_length, min_frequency, min_contexts)\n",
    "    \n",
    "    # store spearman rank correlation for plotting\n",
    "    spearman_rank_correlations  = {min_frequency: spearman_rank_correlation(df)}\n",
    "    print(f'Correlation for {directory_name} corpus with min frequency {min_frequency}: {spearman_rank_correlation(df)}\\n\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output to a file\n",
    "with open(f'../Results/BNC/BNC_{directory_name}.txt', 'w') as f:\n",
    "    f.write(f'\\n{cap.stdout}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (20,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m [correlation\u001b[38;5;241m.\u001b[39mcorrelation \u001b[38;5;28;01mfor\u001b[39;00m correlation \u001b[38;5;129;01min\u001b[39;00m spearman_rank_correlations\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create the plot\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimum Frequency\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpearman Rank Correlation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the x and y values for the plot\n",
    "x = min_freq\n",
    "y = [correlation.correlation for correlation in spearman_rank_correlations.values()]\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Minimum Frequency')\n",
    "plt.ylabel('Spearman Rank Correlation')\n",
    "plt.title('Spearman Rank Correlation vs Minimum Frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
